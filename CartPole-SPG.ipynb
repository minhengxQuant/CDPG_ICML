{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8b8a852-36e3-445b-bef7-695106a98fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.animation as animation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6291488f-164f-47d9-884b-395c29f16a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\" Given a state, return the probability of each action. \"\"\"\n",
    "    def __init__(self, state_dim=4, action_dim=2, hidden_dim=16):\n",
    "        \"\"\" Initialize an Actor object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_dim (int): vector dimension of state vector.\n",
    "            action_dim (int): number of actions.\n",
    "            fc1_dim (int): number of units in the first hidden layer.\n",
    "            fc2_dim (int): number of units in the second hidden layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Given a state, return probability of each action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            x (torch.Tensor): the state tensor of shape [batch_size, state_dim]\n",
    "        \"\"\" \n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # [batch_size, hidden_dim]\n",
    "        return F.softmax(self.fc2(x), dim=1) # [batch_size, action_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d46176-bcbd-4565-aca3-78695b71a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPG:\n",
    "    def __init__(self, state_dim=4, action_dim=2, hidden_dim=8, gamma=0.99, \n",
    "                 actor_lr=2e-2, n_actor=500, n_critic=500, max_t=300, alpha=0.9,\n",
    "                 print_every=1, seed=0):\n",
    "\n",
    "        \"\"\"\n",
    "        n_actor: number of actor network updates\n",
    "        n_critic: In SPG, the number of trajectories per iteration\n",
    "        max_t: length of each trajectory\n",
    "\n",
    "        *patience: no patience required\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input & Output\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Training\n",
    "        self.n_actor = n_actor\n",
    "        self.n_critic = n_critic\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.print_every = print_every\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.device = torch.device('cpu')\n",
    "        self.seed = 0\n",
    "        \n",
    "        # Networks and Optimizers\n",
    "        self.actor_network = Actor(state_dim, action_dim, hidden_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_lr)\n",
    "\n",
    "        # loss functions\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "\n",
    "    # verified; no requirement for batches\n",
    "    def act(self, state):\n",
    "        \"\"\" Given a state vector, return an action randomly.\n",
    "\n",
    "        params\n",
    "        ======\n",
    "            state is represented by an numpy array\n",
    "\n",
    "        Returns\n",
    "        ======\n",
    "            A random action given input state.\n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).unsqueeze(0).float() # Convert state to a torch tensor and add batch dimension\n",
    "        probs = self.actor_network(state).squeeze(0) # Get the action probabilities from the actor network\n",
    "        m = Categorical(probs) # Create a Categorical distribution based on the probabilities\n",
    "        action = m.sample() # Sample an action from the distribution\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "    def CVaR_loss(self, env):\n",
    "        \"\"\" \n",
    "        env: game environment\n",
    "        \"\"\"\n",
    "        ## First off, generate tons of trajectorys\n",
    "        trajectorys = []\n",
    "        probs = []\n",
    "        values = []\n",
    "\n",
    "        # We need to generate number of n_critic trajectorys\n",
    "        for _ in range(self.n_critic):\n",
    "            # initialize current trajectory\n",
    "            trajectory = []\n",
    "            prob = 0\n",
    "            value = 0\n",
    "            \n",
    "            state, _ = env.reset(seed=self.seed) # fix the initial state -> fixed\n",
    "            \n",
    "            for t in range(self.max_t):\n",
    "                action, log_prob = self.act(state) # action no gradient, log_prob with gradient\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                \n",
    "                trajectory.append(state)\n",
    "                trajectory.append(action)\n",
    "                trajectory.append(-reward)\n",
    "                value += self.gamma ** t * (-reward)\n",
    "                prob += log_prob\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "            trajectorys.append(trajectory)\n",
    "            values.append(value)\n",
    "            probs.append(prob)\n",
    "\n",
    "\n",
    "        # Given n_critic trajectorys, and log probabilities of trajectorys and final values\n",
    "        # compute the CVaR Gradient\n",
    "        cvar_grad = 0 # initialization\n",
    "        q_alpha = np.quantile(values, self.alpha, method='inverted_cdf') # compute q_alpha\n",
    "        for i in range(self.n_critic):\n",
    "            if values[i] > q_alpha:\n",
    "                cvar_grad += probs[i] * (values[i] - q_alpha) / self.alpha\n",
    "        \n",
    "        return cvar_grad / self.n_critic\n",
    "\n",
    "    \n",
    "    \n",
    "    def test(self, env):\n",
    "        state, _ = env.reset(seed=self.seed)\n",
    "        if isinstance(state, int):\n",
    "            state = np.array([state])\n",
    "        \n",
    "        score = 0\n",
    "        for t in range(self.max_t):\n",
    "            action, _ = self.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            if isinstance(next_state, int):\n",
    "                next_state = np.array([next_state])\n",
    "            \n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        return score\n",
    "\n",
    "    \n",
    "    def train(self, env):\n",
    "        init_score = self.test(env)\n",
    "        scores_deque = deque(maxlen=5)\n",
    "        scores_deque.append(init_score)\n",
    "        scores = [init_score]\n",
    "        time_count = [0.]\n",
    "        traj_count = [0]\n",
    "\n",
    "        for i_actor in range(self.n_actor):\n",
    "            if i_actor % self.print_every == 0:\n",
    "                print(f'Episode {i_actor}\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "            \n",
    "            start_time = time.time()\n",
    "            loss = self.CVaR_loss(env)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            time_count.append(end_time - start_time)\n",
    "            traj_count.append(self.n_critic)\n",
    "            \n",
    "            # Test One Episode\n",
    "            score = self.test(env)\n",
    "            scores.append(score)\n",
    "            scores_deque.append(score)\n",
    "\n",
    "            if np.mean(scores_deque) >= 1000.0:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_actor-5, np.mean(scores_deque)))\n",
    "                break\n",
    "        \n",
    "        return np.array(scores), np.array(time_count), np.array(traj_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bd12fb5-2389-4c2d-b358-a7c6428eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "068f505c-4294-424e-8de8-54811d91a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPG_Base = SPG(state_dim=4, action_dim=2, hidden_dim=8, gamma=0.99, \n",
    "               actor_lr=1e-2, n_actor=200, n_critic=200, max_t=300, alpha=0.9,\n",
    "               print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fb50e-148c-43b8-bb20-e8b4dd8f3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spg = SPG(state_dim=4, action_dim=2, hidden_dim=8, gamma=0.99, \n",
    "               actor_lr=1e-2, n_actor=100, n_critic=200, max_t=300, alpha=0.9,\n",
    "               print_every=1)\n",
    "spg.actor_network.load_state_dict(SPG_Base.actor_network.state_dict())\n",
    "scores_1, time_count_1, traj_count_1 = spg.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7192ce-f807-41cf-8832-6ae87f30ed9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
