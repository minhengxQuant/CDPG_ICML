{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca18d21-5a8d-4692-9319-78d31baebfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699e610f-ba6e-42ed-afc6-94c3315a19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from os import path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from gymnasium import Env, logger, spaces\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class CliffWalkingEnv(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\", \"ansi\"],\n",
    "        \"render_fps\": 4,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, init_mode = 'original'):\n",
    "        self.shape = (3, 3)\n",
    "        self.start_state_index = np.ravel_multi_index((2, 0), self.shape)\n",
    "        self.init_mode = init_mode\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 4\n",
    "        \n",
    "        # Define Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._cliff[2, 1] = True\n",
    "        \n",
    "        \n",
    "        # Define Locations above cliff\n",
    "        self._abovecliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._abovecliff[1, 1] = True\n",
    "        self.prob = 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate initial state distribution\n",
    "        # We always start in state (3, 0)\n",
    "        if self.init_mode == 'original':\n",
    "            self.initial_state_distrib = np.zeros(self.nS)\n",
    "            self.initial_state_distrib[self.start_state_index] = 1.0\n",
    "        \n",
    "        elif self.init_mode == 'uniform':\n",
    "            self.initial_state_distrib = np.ones(self.nS) / (self.nS-1)\n",
    "            self.initial_state_distrib[-1] = 0\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid Initial Mode')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ## For testing purpose\n",
    "        self.initial_state_distrib_test = np.zeros(self.nS)\n",
    "        self.initial_state_distrib_test[self.start_state_index] = 1.0\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # pygame utils\n",
    "        self.cell_size = (60, 60)\n",
    "        self.window_size = (\n",
    "            self.shape[1] * self.cell_size[1],\n",
    "            self.shape[0] * self.cell_size[0],\n",
    "        )\n",
    "        self.window_surface = None\n",
    "        self.clock = None\n",
    "        self.elf_images = None\n",
    "        self.start_img = None\n",
    "        self.goal_img = None\n",
    "        self.cliff_img = None\n",
    "        self.mountain_bg_img = None\n",
    "        self.near_cliff_img = None\n",
    "        self.tree_img = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _limit_coordinates(self, coord: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prevent the agent from falling out of the grid world.\"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        \"\"\"Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        Args:\n",
    "            current: Current position on the grid as (row, col)\n",
    "            delta: Change in position for transition\n",
    "        Returns:\n",
    "            Tuple of ``(1.0, new_state, reward, terminated)``\n",
    "        \"\"\"\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        \n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, 30.0, False)]\n",
    "        \n",
    "        elif self._abovecliff[tuple(new_position)]:\n",
    "            return [(1 - self.prob, new_state, 10.0, False), (self.prob, self.start_state_index, 30.0, False)]\n",
    "\n",
    "        elif tuple(new_position) == current:\n",
    "            return [(1.0, new_state, 15.0, False)]\n",
    "        \n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_terminated = tuple(new_position) == terminal_state\n",
    "        \n",
    "\n",
    "        return [(1.0, new_state, 10.0, is_terminated)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = np.random.choice(range(len(transitions)), 1, replace=True, p=[t[0] for t in transitions])[0]\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        \n",
    "        return (int(s), r, t, False, {\"prob\": p})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        # original\n",
    "        # self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
    "\n",
    "        self.s = np.random.choice(range(self.nS), 1, replace=True, p=self.initial_state_distrib)[0]\n",
    "        self.lastaction = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def reset_test(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.s = categorical_sample(self.initial_state_distrib_test, self.np_random)\n",
    "        self.lastaction = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef0f62a-b0c5-422e-a9d9-1d4ef14504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def softmax_policy(s, theta):\n",
    "    s = int(s)\n",
    "    theta_exp = np.exp(theta[:, s])\n",
    "    return theta_exp / np.sum(theta_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f71c6c-c9f3-42cc-a436-20f54858878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def log_softmax_policy_grad(s, a, theta):\n",
    "    s = int(s)\n",
    "    grad = np.zeros_like(theta)\n",
    "    probs = softmax_policy(s, theta)\n",
    "    grad[:, s] = -probs\n",
    "    grad[a, s] = grad[a, s] + 1\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db4afa6-56fd-4f9c-b25c-7585682635c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def update_state_action_distribution(state_action_dist, \n",
    "                                     state, action, reward, next_state, next_action, \n",
    "                                     supports, gamma):\n",
    "    vmin, vmax = supports[0], supports[-1]\n",
    "    n_support = len(supports)\n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "    probs = np.zeros(len(supports)) + 1e-16\n",
    "    next_dist = state_action_dist[next_state, next_action]\n",
    "    shifted_supports = reward + gamma * supports\n",
    "    \n",
    "    for j in range(n_support):\n",
    "        \n",
    "        if shifted_supports[j] < vmin:\n",
    "            probs[0] += next_dist[j]\n",
    "        \n",
    "        elif shifted_supports[j] > vmax:\n",
    "            probs[-1] += next_dist[j]\n",
    "        \n",
    "        else:\n",
    "            b = (shifted_supports[j] - vmin) / dz\n",
    "            l = int(np.floor(b))\n",
    "            u = int(np.ceil(b))\n",
    "\n",
    "            probs[l] += next_dist[j] * (u + (l == u) - b) # incase b is an integer, b == u == l\n",
    "            probs[u] += next_dist[j] * (b - l)\n",
    "    \n",
    "    probs = probs / np.sum(probs)\n",
    "    state_action_dist[state, action] = 0.9 * state_action_dist[state, action] + 0.1 * probs\n",
    "\n",
    "    return state_action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4c3a9c-e0a4-447c-b5eb-b778f9e874ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(vmin, vmax, n_support, gamma,\n",
    "                      policy, theta, shape,\n",
    "                      episode, steps):\n",
    "\n",
    "    supports = np.linspace(vmin, vmax, n_support)\n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "    n_state = shape[0] * shape[1]\n",
    "    n_action = 4\n",
    "\n",
    "    state_action_dist = np.ones((n_state, n_action, n_support)) / n_support\n",
    "    state_action_dist[-1, :, 0] = 1.0\n",
    "    state_action_dist[-1, :, 1:] = 0.0\n",
    "\n",
    "    trajectorys = []\n",
    "    for _ in range(episode):\n",
    "\n",
    "        trajectory = []\n",
    "        \n",
    "        train_env = CliffWalkingEnv(init_mode='original')\n",
    "        \n",
    "        state, _ = train_env.reset()\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            action = np.random.choice(range(n_action), 1, replace=True, p=policy(state, theta))[0]\n",
    "            \n",
    "            next_state, reward, done, _, _ = train_env.step(action)\n",
    "\n",
    "            trajectory.append(action)\n",
    "            trajectory.append(reward)\n",
    "            trajectory.append(next_state)\n",
    "            \n",
    "            \n",
    "            next_action = np.random.choice(range(n_action), 1, replace=True, p=policy(next_state, theta))[0]\n",
    "            \n",
    "            state_action_dist = update_state_action_distribution(state_action_dist, \n",
    "                                                                 state, action, reward, next_state, next_action,\n",
    "                                                                 supports, gamma)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                trajectorys.append(np.array(trajectory))\n",
    "                break\n",
    "        \n",
    "        trajectorys.append(np.array(trajectory))\n",
    "    \n",
    "    return state_action_dist, trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a91b79c-b714-4fa5-8a89-19e827d5d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def state_value_distribution(state_action_dist, s, policy, theta):\n",
    "    s = int(s)\n",
    "    dist = state_action_dist[s]\n",
    "    probs = policy(s, theta).reshape(-1, 1)\n",
    "    state_prob = np.sum(dist * probs, axis=0)\n",
    "    return state_prob / np.sum(state_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81952658-04ad-4068-b6f6-8d5f9ebe6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, theta, path_length):\n",
    "    trajectory = []\n",
    "    state, _ = env.reset()\n",
    "    trajectory.append(state)\n",
    "    while len(trajectory) <= 3 * path_length:\n",
    "        action = np.random.choice(range(env.action_space.n), 1, replace=True, p=policy(state, theta))[0]\n",
    "        trajectory.append(action)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        trajectory.append(reward)\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed6f026-ad6a-430d-bc7f-66a3fa6e573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_one_measure_one_pair(measure, r, gamma, supports):\n",
    "    vmin, vmax = supports[0], supports[-1]\n",
    "    n_support = len(supports)\n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "\n",
    "    weights = np.zeros(len(supports))\n",
    "    shifted_supports = r + gamma * supports\n",
    "\n",
    "    for j in range(n_support):\n",
    "        \n",
    "        if shifted_supports[j] < vmin:\n",
    "            weights[0] += measure[j]\n",
    "        \n",
    "        elif shifted_supports[j] > vmax:\n",
    "            weights[-1] += measure[j]\n",
    "\n",
    "        else:\n",
    "            b = (shifted_supports[j] - vmin) / dz\n",
    "            l = int(np.floor(b))\n",
    "            u = int(np.ceil(b))\n",
    "    \n",
    "            weights[l] += measure[j] * (u + (l == u) - b)\n",
    "            weights[u] += measure[j] * (b - l)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8693ea-6b2c-4e40-ab80-0cfc4205f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_all_measure_one_pair(measures, r, gamma, supports):\n",
    "    n_state, n_action, n_sup = measures.shape\n",
    "    for s in range(n_state):\n",
    "        for a in range(n_action):\n",
    "            measures[s, a] = pushforward_one_measure_one_pair(measures[s, a], r, gamma, supports)\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c77d479-e69d-418e-8c82-85a8d3045794",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_one_trajectory(measures, trajectory, gamma, supports):\n",
    "    if len(trajectory) != 0:\n",
    "        trajectory_copy = np.copy(trajectory)\n",
    "\n",
    "        for i in range(len(trajectory_copy) // 3):\n",
    "            trajectory_copy[i*3+2] = trajectory_copy[i*3+2] * gamma ** 0\n",
    "\n",
    "        for r in trajectory_copy[::-1][0:len(trajectory_copy):3]:\n",
    "            measures = pushforward_all_measure_one_pair(measures, r, gamma, supports)\n",
    "    \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3100d6b3-faac-45cd-afd2-23a57a354256",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports):\n",
    "    log_grad_weight_state_dist = []\n",
    "    for s in range(9):\n",
    "        res = np.zeros((4, 9, len(supports)))\n",
    "        \n",
    "        for a in range(4):\n",
    "            prob = policy(s, theta)[a]\n",
    "            res = res + prob * log_policy_grad(s, a, theta)[:, :, np.newaxis] * state_action_dist[s, a].reshape(1, 1, -1) # broadcast\n",
    "        log_grad_weight_state_dist.append(res)\n",
    "    \n",
    "    res = np.zeros((4, 9, len(supports)), dtype=np.float64)\n",
    "\n",
    "    # For each time step, the trajectory should be like: (s_0),(s_0, a_0, r_0, s_1), (s_0, a_0, r_0, s_1, a_1, r_1, s_2) ....\n",
    "    for i in range((len(trajectory)-1) // 3 + 1):\n",
    "        sub_trajectory = trajectory[:3*i+1]\n",
    "        s = int(sub_trajectory[-1]) # The target initial distribution before pushforwards\n",
    "        # the input trajectory should not include the final state, a.k.a, (), (s_0, a_0, r_0), (s_0, a_0, r_0, s_1, a_1, r_1)\n",
    "        res = res + pushforward_one_trajectory(log_grad_weight_state_dist[s], sub_trajectory[:-1], gamma, supports)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87200b20-aa9f-4642-ae03-c57ee6372a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVaR_policy_gradient(state_action_dist, trajectory,\n",
    "                         policy, log_policy_grad, theta, gamma, \n",
    "                         supports, alpha):\n",
    "\n",
    "    # Step 1: Compute the distribution at initial state\n",
    "    init_state = int(trajectory[0])\n",
    "    state_value_dist = state_value_distribution(state_action_dist, init_state, policy, theta)\n",
    "\n",
    "    # Step 2: Compute the alpha-quantile and filter out tail supports\n",
    "    if_tail = np.cumsum(state_value_dist) > alpha\n",
    "    q_alpha = supports[if_tail][0]\n",
    "    \n",
    "    tail_supports = supports[if_tail]\n",
    "    tail_supports_idx = np.where(if_tail)[0]\n",
    "    tail_prob = state_value_dist[if_tail]\n",
    "    \n",
    "    # Step 3: Compute the gradient: 4 x 9 x n_support\n",
    "    sa_grad = state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports)\n",
    "    \n",
    "    # Step 4: Compute CVaR policy gradient\n",
    "    cvar_grad = np.zeros_like(theta)\n",
    "    for i in range(len(tail_supports_idx)):\n",
    "        cvar_grad = cvar_grad + sa_grad[:, :, i] * (tail_supports[i] - q_alpha)\n",
    "    \n",
    "    cvar_grad = cvar_grad / (1 - alpha)\n",
    "    \n",
    "    return cvar_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e7d65b2-89a1-4032-96f9-cb3714709805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expectation_policy_gradient(state_action_dist, trajectory, \n",
    "                                policy, log_policy_grad, theta, gamma, \n",
    "                                supports):\n",
    "    # Step 1: Compute the distribution at initial state\n",
    "    init_state = int(trajectory[0])\n",
    "    state_value_dist = state_value_distribution(state_action_dist, init_state, policy, theta)\n",
    "\n",
    "    # Step 2: Compute the gradient: 4 x 9 x n_support\n",
    "    sa_grad = state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports)\n",
    "\n",
    "    # Step 3: Compute Expectation Policy Gradient\n",
    "    grad = np.zeros_like(theta)\n",
    "    for i in range(len(supports)):\n",
    "        grad = grad + sa_grad[:, :, i] * supports[i]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5cf3b4-5562-45ec-90c2-35eb7325351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_based_CVaR_gradient(trajectorys, policy, log_policy_grad, theta, gamma, alpha):\n",
    "    \"\"\"\n",
    "    trajectorys: a list of trajectorys -> must be sampled at the same time\n",
    "    \"\"\"\n",
    "    ## Compute Value Distributions\n",
    "    z_ls = []\n",
    "    for trajectory in trajectorys:\n",
    "        z = np.array(trajectory[2::3])\n",
    "        # print(z)\n",
    "        discount = np.array([gamma ** i for i in range(len(z))])\n",
    "        # print(discount)\n",
    "        z_ls.append(np.sum(discount * z))\n",
    "    # print(z_ls)\n",
    "    ## alpha-quantile\n",
    "    q_alpha = np.quantile(z_ls, alpha, method='inverted_cdf')\n",
    "    # print(q_alpha)\n",
    "    \n",
    "    ## Compute the trajectory probability\n",
    "    def log_prob_traj(trajectory):\n",
    "        res = 0\n",
    "        sub_trajectory = trajectory[:-1]\n",
    "        s_ls = sub_trajectory[::3]\n",
    "        a_ls = sub_trajectory[1::3]\n",
    "        for s, a in zip(s_ls, a_ls):\n",
    "            res += log_policy_grad(s, int(a), theta)\n",
    "        return res\n",
    "    \n",
    "    # Compute the gradient\n",
    "    grad = 0\n",
    "    for i in range(len(trajectorys)):\n",
    "        if z_ls[i] > q_alpha:\n",
    "            res = log_prob_traj(trajectorys[i]) * (z_ls[i] - q_alpha)\n",
    "            # print(res)\n",
    "            grad = grad + res / len(trajectorys)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e2968c-2c42-4118-86dd-8a947a8d10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(theta, lr, alpha=0.9, episode=5000, \n",
    "          vmin=0, vmax=2000, n_support=51, \n",
    "          gamma=0.95, policy=softmax_policy, log_policy_grad=log_softmax_policy_grad, \n",
    "          steps = 100, path_length=50, \n",
    "          num_train=50, shape=(3, 3)):\n",
    "    \n",
    "    \n",
    "    supports = np.linspace(vmin, vmax, n_support)\n",
    "    theta_ls = [np.copy(theta)]\n",
    "    grad_ls = []\n",
    "    \n",
    "    for n_train in tqdm(range(num_train)):\n",
    "        ###################### Test Starts ######################\n",
    "        total_cost = 0\n",
    "        path = []\n",
    "        env = CliffWalkingEnv(init_mode='original')\n",
    "        env.reset()\n",
    "        state, _ = env.reset_test()\n",
    "        for n_step in range(10):\n",
    "            path.append(state)\n",
    "            action = np.argmax(policy(state, theta))\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_cost += reward\n",
    "            if done:\n",
    "                path.append(state)\n",
    "                break\n",
    "        \n",
    "        if (n_train+1) % 2 == 0:\n",
    "            print('Verbose One Episode Policy: \\n')\n",
    "            print(f'State 6: {policy(6, theta)}')\n",
    "            print(f'State 3: {policy(3, theta)}')\n",
    "            print(f'State 0: {policy(0, theta)}')\n",
    "            print(f'State 1: {policy(1, theta)}')\n",
    "            print(f'State 2: {policy(2, theta)}')\n",
    "            print(f'State 4: {policy(4, theta)}')\n",
    "            print(f'State 5: {policy(5, theta)}')\n",
    "        \n",
    "            if done:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is Reached \\n')\n",
    "            \n",
    "            else:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is NOT Reached \\n')\n",
    "\n",
    "        env.close()\n",
    "        ###################### Test Ends ######################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Policy Evaluation Starts ######################\n",
    "        state_action_dist, trajectorys = policy_evaluation(vmin, vmax, n_support, gamma, \n",
    "                                              policy, theta, shape, \n",
    "                                              episode, steps)\n",
    "        ###################### Policy Evaluation Ends ######################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Policy Improvement Starts ######################\n",
    "        grad = 0\n",
    "        # trajectorys = [sample_trajectory(CliffWalkingEnv(init_mode='original'), policy, theta, path_length) for _ in range(n_trajectory)]\n",
    "        for trajectory in trajectorys:\n",
    "            grad += CVaR_policy_gradient(state_action_dist, trajectory, \n",
    "                                        policy, log_policy_grad, theta, gamma, \n",
    "                                        supports, alpha)\n",
    "        grad /= len(trajectorys)\n",
    "        theta -= lr * (grad / np.linalg.norm(grad))   ## 归一化, for a fair comparison\n",
    "        ###################### Policy Improvement Ends ######################\n",
    "        \n",
    "        \n",
    "        # Storage\n",
    "        theta_ls.append(np.copy(theta))\n",
    "        grad_ls.append(grad)\n",
    "        print(np.linalg.norm(grad, ord=np.inf))\n",
    "    \n",
    "    return theta, theta_ls, grad_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433648e-fce1-4dbe-b3d7-ab07a8bfba70",
   "metadata": {},
   "source": [
    "## Risk Averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e44e014-30b2-47fd-be5d-723dcd4f4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(49)\n",
    "THETA_AVERSE = np.random.uniform(-5, 5, (4, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5c072-7002-42b2-8a78-a69cfc974652",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_90_formal = []\n",
    "grad_90_formal = []\n",
    "\n",
    "for num in range(1):\n",
    "    print(f'Number of Training: {num}')\n",
    "    _, theta_ls, grad_ls = train(theta=np.copy(THETA_AVERSE), lr=0.3, alpha=0.9, episode=1000, num_train=100)\n",
    "    theta_90_formal.append(theta_ls)\n",
    "    grad_90_formal.append(grad_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a06d58-a9e2-4919-8048-7dd504d9672c",
   "metadata": {},
   "source": [
    "## Risk Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e9e52ac-fe5a-4679-a0a9-6015cac43ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "THETA_NEUTRAL = np.random.uniform(-5, 5, (4, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792a61f-79f8-43e5-85b7-c852c7f1d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0_formal = []\n",
    "grad_0_formal = []\n",
    "\n",
    "for num in range(10):\n",
    "    print(f'Number of Training: {num}')\n",
    "    _, theta_ls, grad_ls = train(theta=np.copy(THETA_NEUTRAL), lr=0.3, alpha=0.0, episode=1000, num_train=100)\n",
    "    theta_0_formal.append(theta_ls)\n",
    "    grad_0_formal.append(grad_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
