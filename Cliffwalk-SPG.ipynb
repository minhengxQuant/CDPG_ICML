{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca18d21-5a8d-4692-9319-78d31baebfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699e610f-ba6e-42ed-afc6-94c3315a19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env, logger, spaces\n",
    "from gym.envs.toy_text.utils import categorical_sample\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class CliffWalkingEnv(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\", \"ansi\"],\n",
    "        \"render_fps\": 4,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, init_mode = 'original'):\n",
    "        self.shape = (3, 3)\n",
    "        self.start_state_index = np.ravel_multi_index((2, 0), self.shape)\n",
    "        self.init_mode = init_mode\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 4\n",
    "        \n",
    "        # Define Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._cliff[2, 1] = True\n",
    "        \n",
    "        \n",
    "        # Define Locations above cliff\n",
    "        self._abovecliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._abovecliff[1, 1] = True\n",
    "        self.prob = 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate initial state distribution\n",
    "        # We always start in state (3, 0)\n",
    "        if self.init_mode == 'original':\n",
    "            self.initial_state_distrib = np.zeros(self.nS)\n",
    "            self.initial_state_distrib[self.start_state_index] = 1.0\n",
    "        \n",
    "        elif self.init_mode == 'uniform':\n",
    "            self.initial_state_distrib = np.ones(self.nS) / (self.nS-1)\n",
    "            self.initial_state_distrib[-1] = 0\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid Initial Mode')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ## For testing purpose\n",
    "        self.initial_state_distrib_test = np.zeros(self.nS)\n",
    "        self.initial_state_distrib_test[self.start_state_index] = 1.0\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # pygame utils\n",
    "        self.cell_size = (60, 60)\n",
    "        self.window_size = (\n",
    "            self.shape[1] * self.cell_size[1],\n",
    "            self.shape[0] * self.cell_size[0],\n",
    "        )\n",
    "        self.window_surface = None\n",
    "        self.clock = None\n",
    "        self.elf_images = None\n",
    "        self.start_img = None\n",
    "        self.goal_img = None\n",
    "        self.cliff_img = None\n",
    "        self.mountain_bg_img = None\n",
    "        self.near_cliff_img = None\n",
    "        self.tree_img = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _limit_coordinates(self, coord: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prevent the agent from falling out of the grid world.\"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        \"\"\"Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        Args:\n",
    "            current: Current position on the grid as (row, col)\n",
    "            delta: Change in position for transition\n",
    "        Returns:\n",
    "            Tuple of ``(1.0, new_state, reward, terminated)``\n",
    "        \"\"\"\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        \n",
    "        # if fall off the cliff, go back to the start_state_index\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, 30.0, False)]\n",
    "        \n",
    "        # if above the cliff, with some probability, it falls off or safe\n",
    "        elif self._abovecliff[tuple(new_position)]:\n",
    "            return [(1 - self.prob, new_state, 10.0, False), (self.prob, self.start_state_index, 30.0, False)]\n",
    "\n",
    "        # hit the wall\n",
    "        elif tuple(new_position) == current:\n",
    "            return [(1.0, new_state, 15.0, False)]\n",
    "        \n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_terminated = tuple(new_position) == terminal_state\n",
    "        \n",
    "\n",
    "        return [(1.0, new_state, 10.0, is_terminated)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = np.random.choice(range(len(transitions)), 1, replace=True, p=[t[0] for t in transitions])[0]\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        \n",
    "        return (int(s), r, t, False, {\"prob\": p})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        # original\n",
    "        # self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
    "\n",
    "        self.s = np.random.choice(range(self.nS), 1, replace=True, p=self.initial_state_distrib)[0]\n",
    "        self.lastaction = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def reset_test(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.s = categorical_sample(self.initial_state_distrib_test, self.np_random)\n",
    "        self.lastaction = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef0f62a-b0c5-422e-a9d9-1d4ef14504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def softmax_policy(s, theta):\n",
    "    \"\"\" Given a state, compute probability of each action \"\"\"\n",
    "\n",
    "    \"\"\" \n",
    "    Arguments\n",
    "        s[integer]: Index of state\n",
    "        theta[np.ndarray]: Policy parameter of shape (n_actions, n_states)\n",
    "\n",
    "    Returns:\n",
    "        The probability of each action at state s -> (n_action, )\n",
    "    \"\"\"\n",
    "    s = int(s)\n",
    "    theta_exp = np.exp(theta[:, s])\n",
    "    return theta_exp / np.sum(theta_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f71c6c-c9f3-42cc-a436-20f54858878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def log_softmax_policy_grad(s, a, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of log softmax policy with respect to (a, s) [theta_{a, s}]\n",
    "    \"\"\"\n",
    "    s = int(s)\n",
    "    grad = np.zeros_like(theta)\n",
    "    probs = softmax_policy(s, theta)\n",
    "    \n",
    "    # only consider s-th column, all rows are -\\pi(i|s)\n",
    "    grad[:, s] = -probs\n",
    "    \n",
    "    # Consider a-th row, there will be additional +1\n",
    "    grad[a, s] = grad[a, s] + 1\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db4afa6-56fd-4f9c-b25c-7585682635c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def update_state_action_distribution(state_action_dist,  # previous distribution\n",
    "                                     state, action, reward, next_state, next_action,  # Give a SARSA pair\n",
    "                                     supports, gamma):\n",
    "    \"\"\" Given a SARSA pair, update the state-action distribution under Bellman equation \"\"\"\n",
    "\n",
    "    vmin, vmax = supports[0], supports[-1]\n",
    "    n_support = len(supports)                \n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "\n",
    "    probs = np.zeros(len(supports)) + 1e-16\n",
    "\n",
    "    next_dist = state_action_dist[next_state, next_action]\n",
    "    \n",
    "    \n",
    "    shifted_supports = reward + gamma * supports\n",
    "    \n",
    "    for j in range(n_support):\n",
    "        \n",
    "        if shifted_supports[j] < vmin:\n",
    "            probs[0] += next_dist[j]\n",
    "        \n",
    "        elif shifted_supports[j] > vmax:\n",
    "            probs[-1] += next_dist[j]\n",
    "        \n",
    "        else:\n",
    "            b = (shifted_supports[j] - vmin) / dz\n",
    "            l = int(np.floor(b))\n",
    "            u = int(np.ceil(b))\n",
    "\n",
    "            probs[l] += next_dist[j] * (u + (l == u) - b) # incase b is an integer, b == u == l\n",
    "            probs[u] += next_dist[j] * (b - l)\n",
    "    \n",
    "    \n",
    "    probs = probs / np.sum(probs)\n",
    "    state_action_dist[state, action] = 0.9 * state_action_dist[state, action] + 0.1 * probs\n",
    "\n",
    "    return state_action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd4c3a9c-e0a4-447c-b5eb-b778f9e874ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(vmin, vmax, n_support, gamma,\n",
    "                      policy, theta, shape,\n",
    "                      episode, steps):\n",
    "\n",
    "    supports = np.linspace(vmin, vmax, n_support)\n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "    n_state = shape[0] * shape[1]\n",
    "    n_action = 4\n",
    "\n",
    "    state_action_dist = np.ones((n_state, n_action, n_support)) / n_support\n",
    "    state_action_dist[-1, :, 0] = 1.0      # last state, all actions, first cost\n",
    "    state_action_dist[-1, :, 1:] = 0.0     # other points are zeros\n",
    "\n",
    "    trajectorys = []\n",
    "\n",
    "    # Start Several Games for Policy Evaluation\n",
    "    for _ in range(episode):\n",
    "\n",
    "        trajectory = []\n",
    "        \n",
    "        train_env = CliffWalkingEnv(init_mode='original')\n",
    "        \n",
    "        state, _ = train_env.reset()\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            action = np.random.choice(range(n_action), 1, replace=True, p=policy(state, theta))[0]\n",
    "            \n",
    "            next_state, reward, done, _, _ = train_env.step(action)\n",
    "\n",
    "            trajectory.append(action)\n",
    "            trajectory.append(reward)\n",
    "            trajectory.append(next_state)\n",
    "            \n",
    "            \n",
    "            next_action = np.random.choice(range(n_action), 1, replace=True, p=policy(next_state, theta))[0]\n",
    "            \n",
    "            state_action_dist = update_state_action_distribution(state_action_dist, \n",
    "                                                                 state, action, reward, next_state, next_action,\n",
    "                                                                 supports, gamma)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                trajectorys.append(np.array(trajectory))\n",
    "                break\n",
    "        \n",
    "        trajectorys.append(np.array(trajectory))\n",
    "    \n",
    "    return state_action_dist, trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a91b79c-b714-4fa5-8a89-19e827d5d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def state_value_distribution(state_action_dist, s, policy, theta):\n",
    "    \"\"\" Given a state-action distribution [state, action, supports], \n",
    "        compute the state value distribution at target state 's' \"\"\"\n",
    "    # state action distribution: [state x action x support]\n",
    "    s = int(s)\n",
    "    \n",
    "    # action x support\n",
    "    dist = state_action_dist[s]  # retrieve each action distribution at target state s\n",
    "    \n",
    "    # action x 1\n",
    "    probs = policy(s, theta).reshape(-1, 1)  # Compute the probability of each action \n",
    "    \n",
    "    # action x support\n",
    "    state_prob = np.sum(dist * probs, axis=0)  # Action probability weighted average\n",
    "    \n",
    "    return state_prob / np.sum(state_prob)  # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81952658-04ad-4068-b6f6-8d5f9ebe6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, theta, path_length):\n",
    "    \"\"\" Sample a trajectory looks like (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T) \"\"\"\n",
    "    trajectory = []\n",
    "    state, _ = env.reset()\n",
    "    trajectory.append(state)\n",
    "    while len(trajectory) <= 3 * path_length:\n",
    "        action = np.random.choice(range(env.action_space.n), 1, replace=True, p=policy(state, theta))[0]\n",
    "        trajectory.append(action)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        trajectory.append(reward)\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed6f026-ad6a-430d-bc7f-66a3fa6e573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_one_measure_one_pair(measure, r, gamma, supports):\n",
    "    \"\"\" Given a measure (measure values at discrete points), \n",
    "        by receiving one reward, \n",
    "        conduct one-time pushforward with projection \"\"\"\n",
    "    \n",
    "    vmin, vmax = supports[0], supports[-1]\n",
    "    n_support = len(supports)\n",
    "    dz = (vmax - vmin) / (n_support - 1)\n",
    "\n",
    "    weights = np.zeros(len(supports)) # Initialization\n",
    "    shifted_supports = r + gamma * supports\n",
    "\n",
    "    for j in range(n_support):\n",
    "        \n",
    "        if shifted_supports[j] < vmin:\n",
    "            weights[0] += measure[j]\n",
    "        \n",
    "        elif shifted_supports[j] > vmax:\n",
    "            weights[-1] += measure[j]\n",
    "\n",
    "        else:\n",
    "            b = (shifted_supports[j] - vmin) / dz\n",
    "            l = int(np.floor(b))\n",
    "            u = int(np.ceil(b))\n",
    "    \n",
    "            weights[l] += measure[j] * (u + (l == u) - b) # In case b = l = u is an integer\n",
    "            weights[u] += measure[j] * (b - l)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8693ea-6b2c-4e40-ab80-0cfc4205f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_all_measure_one_pair(measures, r, gamma, supports):\n",
    "    \"\"\" Given a whole measure (state, num_supports), \n",
    "        push forward all single measures inside \"\"\"\n",
    "    n_state, n_action, n_sup = measures.shape\n",
    "    for s in range(n_state):\n",
    "        for a in range(n_action):\n",
    "            measures[s, a] = pushforward_one_measure_one_pair(measures[s, a], r, gamma, supports)\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c77d479-e69d-418e-8c82-85a8d3045794",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def pushforward_one_trajectory(measures, trajectory, gamma, supports):\n",
    "    \"\"\" Given a trajectory, Pushforward the whole measure along the trajectory \"\"\"\n",
    "    \"\"\" The trajectory does not contain the final state \"\"\"\n",
    "    # if length of trajectory is 0, then no pushforward operator is required\n",
    "    if len(trajectory) != 0:\n",
    "        # (s1, a1, g^0 * r1, s2, a2, g^1 * r2)\n",
    "        # (r2, a2, s2, r1, a1, s1) -> (g^1 * r2, r1)\n",
    "        trajectory_copy = np.copy(trajectory)\n",
    "\n",
    "        # Discount Reward\n",
    "        for i in range(len(trajectory_copy) // 3):\n",
    "            trajectory_copy[i*3+2] = trajectory_copy[i*3+2] * gamma ** 0\n",
    "\n",
    "        # Backward Pushforward\n",
    "        for r in trajectory_copy[::-1][0:len(trajectory_copy):3]:\n",
    "            measures = pushforward_all_measure_one_pair(measures, r, gamma, supports)\n",
    "    \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3100d6b3-faac-45cd-afd2-23a57a354256",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports):\n",
    "    \"\"\" Given a trajectory, estimate the state value distribution gradient \"\"\"\n",
    "\n",
    "    # trajectory = np.array(trajectory)\n",
    "    # Step 1: Compute the log-policy-gradient weighted state value distribution\n",
    "    log_grad_weight_state_dist = []\n",
    "    for s in range(9):\n",
    "        res = np.zeros((4, 9, len(supports)))\n",
    "        \n",
    "        for a in range(4):\n",
    "            prob = policy(s, theta)[a]\n",
    "            res = res + prob * log_policy_grad(s, a, theta)[:, :, np.newaxis] * state_action_dist[s, a].reshape(1, 1, -1) # broadcast\n",
    "        log_grad_weight_state_dist.append(res)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Step 2: Given a trajectory, Compute the gradient\n",
    "    res = np.zeros((4, 9, len(supports)), dtype=np.float64)\n",
    "\n",
    "    # For each time step, the trajectory should be like: (s_0),(s_0, a_0, r_0, s_1), (s_0, a_0, r_0, s_1, a_1, r_1, s_2) ....\n",
    "    for i in range((len(trajectory)-1) // 3 + 1):\n",
    "        sub_trajectory = trajectory[:3*i+1]\n",
    "        s = int(sub_trajectory[-1]) # The target initial distribution before pushforwards\n",
    "        # the input trajectory should not include the final state, a.k.a, (), (s_0, a_0, r_0), (s_0, a_0, r_0, s_1, a_1, r_1)\n",
    "        res = res + pushforward_one_trajectory(log_grad_weight_state_dist[s], sub_trajectory[:-1], gamma, supports)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87200b20-aa9f-4642-ae03-c57ee6372a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVaR_policy_gradient(state_action_dist, trajectory,\n",
    "                         policy, log_policy_grad, theta, gamma, \n",
    "                         supports, alpha):\n",
    "\n",
    "    # Step 1: Compute the distribution at initial state\n",
    "    init_state = int(trajectory[0])\n",
    "    state_value_dist = state_value_distribution(state_action_dist, init_state, policy, theta)\n",
    "\n",
    "    # Step 2: Compute the alpha-quantile and filter out tail supports\n",
    "    if_tail = np.cumsum(state_value_dist) > alpha\n",
    "    q_alpha = supports[if_tail][0]\n",
    "    \n",
    "    tail_supports = supports[if_tail]\n",
    "    tail_supports_idx = np.where(if_tail)[0]\n",
    "    tail_prob = state_value_dist[if_tail]\n",
    "    \n",
    "    # Step 3: Compute the gradient: 4 x 9 x n_support\n",
    "    sa_grad = state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports)\n",
    "    \n",
    "    # Step 4: Compute CVaR policy gradient\n",
    "    cvar_grad = np.zeros_like(theta)\n",
    "    for i in range(len(tail_supports_idx)):\n",
    "        cvar_grad = cvar_grad + sa_grad[:, :, i] * (tail_supports[i] - q_alpha)\n",
    "    \n",
    "    cvar_grad = cvar_grad / (1 - alpha)\n",
    "    \n",
    "    return cvar_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7d65b2-89a1-4032-96f9-cb3714709805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expectation_policy_gradient(state_action_dist, trajectory, \n",
    "                                policy, log_policy_grad, theta, gamma, \n",
    "                                supports):\n",
    "    # Step 1: Compute the distribution at initial state\n",
    "    init_state = int(trajectory[0])\n",
    "    state_value_dist = state_value_distribution(state_action_dist, init_state, policy, theta)\n",
    "\n",
    "    # Step 2: Compute the gradient: 4 x 9 x n_support\n",
    "    sa_grad = state_value_dist_grad(state_action_dist, trajectory, policy, log_policy_grad, theta, gamma, supports)\n",
    "\n",
    "    # Step 3: Compute Expectation Policy Gradient\n",
    "    grad = np.zeros_like(theta)\n",
    "    for i in range(len(supports)):\n",
    "        grad = grad + sa_grad[:, :, i] * supports[i]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf5cf3b4-5562-45ec-90c2-35eb7325351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_based_CVaR_gradient(trajectorys, policy, log_policy_grad, theta, gamma, alpha):\n",
    "    \"\"\"\n",
    "    trajectorys: a list of trajectorys -> must be sampled at the same time\n",
    "    \"\"\"\n",
    "    ## Compute Value Distributions\n",
    "    z_ls = []\n",
    "    for trajectory in trajectorys:\n",
    "        z = np.array(trajectory[2::3])\n",
    "        # print(z)\n",
    "        discount = np.array([gamma ** i for i in range(len(z))])\n",
    "        # print(discount)\n",
    "        z_ls.append(np.sum(discount * z))\n",
    "    # print(z_ls)\n",
    "    ## alpha-quantile\n",
    "    q_alpha = np.quantile(z_ls, alpha, method='inverted_cdf')\n",
    "    # print(q_alpha)\n",
    "    \n",
    "    ## Compute the trajectory probability\n",
    "    def log_prob_traj(trajectory):\n",
    "        res = 0\n",
    "        sub_trajectory = trajectory[:-1]\n",
    "        s_ls = sub_trajectory[::3]\n",
    "        a_ls = sub_trajectory[1::3]\n",
    "        for s, a in zip(s_ls, a_ls):\n",
    "            res += log_policy_grad(s, int(a), theta)\n",
    "        return res\n",
    "    \n",
    "    # Compute the gradient\n",
    "    grad = 0\n",
    "    for i in range(len(trajectorys)):\n",
    "        if z_ls[i] > q_alpha:\n",
    "            res = log_prob_traj(trajectorys[i]) * (z_ls[i] - q_alpha)\n",
    "            # print(res)\n",
    "            grad = grad + res / len(trajectorys)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f7e2968c-2c42-4118-86dd-8a947a8d10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(theta, lr, alpha=0.9, episode=5000, \n",
    "          vmin=0, vmax=2000, n_support=51, \n",
    "          gamma=0.95, policy=softmax_policy, log_policy_grad=log_softmax_policy_grad, \n",
    "          steps = 100, path_length=50, \n",
    "          num_train=50, shape=(3, 3)):\n",
    "    \n",
    "    \n",
    "    supports = np.linspace(vmin, vmax, n_support)\n",
    "    theta_ls = [np.copy(theta)]\n",
    "    grad_ls = []\n",
    "    \n",
    "    for n_train in tqdm(range(num_train)):\n",
    "        ###################### Test Starts ######################\n",
    "        total_cost = 0\n",
    "        path = []\n",
    "        env = CliffWalkingEnv(init_mode='original')\n",
    "        env.reset()\n",
    "        state, _ = env.reset_test()\n",
    "        for n_step in range(10):\n",
    "            path.append(state)\n",
    "            action = np.argmax(policy(state, theta))\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_cost += reward\n",
    "            if done:\n",
    "                path.append(state)\n",
    "                break\n",
    "        \n",
    "        if (n_train+1) % 2 == 0:\n",
    "            print('Verbose One Episode Policy: \\n')\n",
    "            print(f'State 6: {policy(6, theta)}')\n",
    "            print(f'State 3: {policy(3, theta)}')\n",
    "            print(f'State 0: {policy(0, theta)}')\n",
    "            print(f'State 1: {policy(1, theta)}')\n",
    "            print(f'State 2: {policy(2, theta)}')\n",
    "            print(f'State 4: {policy(4, theta)}')\n",
    "            print(f'State 5: {policy(5, theta)}')\n",
    "        \n",
    "            if done:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is Reached \\n')\n",
    "            \n",
    "            else:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is NOT Reached \\n')\n",
    "\n",
    "        env.close()\n",
    "        ###################### Test Ends ######################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Policy Evaluation Starts ######################\n",
    "        state_action_dist, trajectorys = policy_evaluation(vmin, vmax, n_support, gamma, \n",
    "                                              policy, theta, shape, \n",
    "                                              episode, steps)\n",
    "        ###################### Policy Evaluation Ends ######################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################### Policy Improvement Starts ######################\n",
    "        grad = 0\n",
    "        # trajectorys = [sample_trajectory(CliffWalkingEnv(init_mode='original'), policy, theta, path_length) for _ in range(n_trajectory)]\n",
    "        for trajectory in trajectorys:\n",
    "            grad += CVaR_policy_gradient(state_action_dist, trajectory, \n",
    "                                        policy, log_policy_grad, theta, gamma, \n",
    "                                        supports, alpha)\n",
    "        grad /= len(trajectorys)\n",
    "        theta -= lr * (grad / np.linalg.norm(grad))   ## 归一化, for a fair comparison\n",
    "        ###################### Policy Improvement Ends ######################\n",
    "        \n",
    "        \n",
    "        # Storage\n",
    "        theta_ls.append(np.copy(theta))\n",
    "        grad_ls.append(grad)\n",
    "        print(np.linalg.norm(grad))\n",
    "    \n",
    "    return theta, theta_ls, grad_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "edb022d9-de6e-48f8-bcc2-fc2b58c12e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_based_train(theta, lr, alpha=0.9, episode=5000, \n",
    "                        gamma=0.95, policy=softmax_policy, log_policy_grad=log_softmax_policy_grad, \n",
    "                        path_length=50, num_train=50, shape=(3, 3)):\n",
    "\n",
    "    \n",
    "    theta_ls = [np.copy(theta)]\n",
    "    grad_ls = []\n",
    "    for n_train in tqdm(range(num_train)):\n",
    "\n",
    "        ################################# Test Cost #################################\n",
    "        total_cost = 0\n",
    "        path = []\n",
    "        env = CliffWalkingEnv(init_mode='original')\n",
    "        env.reset()\n",
    "        state, _ = env.reset_test()\n",
    "        for n_step in range(10):\n",
    "            path.append(state)\n",
    "            action = np.argmax(policy(state, theta))\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_cost += reward\n",
    "            if done:\n",
    "                path.append(state)\n",
    "                break\n",
    "        \n",
    "        if (n_train+1) % 2 == 0:\n",
    "            print('Verbose One Episode Policy: \\n')\n",
    "            print(f'State 6: {policy(6, theta)}')\n",
    "            print(f'State 3: {policy(3, theta)}')\n",
    "            print(f'State 0: {policy(0, theta)}')\n",
    "            print(f'State 1: {policy(1, theta)}')\n",
    "            print(f'State 2: {policy(2, theta)}')\n",
    "            print(f'State 4: {policy(4, theta)}')\n",
    "            print(f'State 5: {policy(5, theta)}')\n",
    "        \n",
    "            if done:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is Reached \\n')\n",
    "            \n",
    "            else:\n",
    "                print(f'Path is {path}, Total Cost is: {total_cost}, The Goal is NOT Reached \\n')\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        \n",
    "        ################################# Policy Evaluation #################################\n",
    "        trajectorys = [sample_trajectory(CliffWalkingEnv(init_mode='original'), policy, theta, path_length) for _ in range(episode)]\n",
    "        grad = sample_based_CVaR_gradient(trajectorys, policy, log_policy_grad, theta, gamma, alpha)\n",
    "        if np.linalg.norm(grad) > 0:\n",
    "            theta -= lr * (grad / np.linalg.norm(grad))\n",
    "        else:\n",
    "            theta -= lr * grad\n",
    "        \n",
    "        print(np.linalg.norm(grad))\n",
    "        theta_ls.append(np.copy(theta))\n",
    "        grad_ls.append(grad)\n",
    "    \n",
    "    return theta, theta_ls, grad_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a6cad2c0-f089-4714-8d97-d25f7fe8e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(49)\n",
    "THETA = np.random.uniform(-5, 5, (4, 9)) + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7ead9-bea7-484e-a675-afe18e000683",
   "metadata": {},
   "source": [
    "# CDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d80bb6-84f9-4d49-b058-9c21b8651222",
   "metadata": {},
   "source": [
    "## 100 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a4947-d748-4261-8436-20a0e9c2aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_100_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=100, num_train=100)\n",
    "    our_100_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefaa110-bebf-4fa3-8553-92ab57aa1afc",
   "metadata": {},
   "source": [
    "## 150 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979da2e-d270-450c-9f45-2b211c129eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_150_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=150, num_train=100)\n",
    "    our_150_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8e613-c562-4d2c-ad69-3b79e2b261d8",
   "metadata": {},
   "source": [
    "## 200 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc67c1-fe5f-405d-93f0-fd0a3d63708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_200_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=200, num_train=100)\n",
    "    our_200_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48be55c-3bd1-41eb-93fc-c8f980c4a0d4",
   "metadata": {},
   "source": [
    "## 500 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b471b04-0ad4-4cf0-9cf8-f1b661a42523",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_500_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=500, num_train=100)\n",
    "    our_500_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a81682-998b-4431-a9f6-53eb63ea8570",
   "metadata": {},
   "source": [
    "## 1000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af6def-ff88-4d8c-90cf-e78438b57be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_1000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=1000, num_train=100)\n",
    "    our_1000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142ba8f-272d-4831-a135-d857808cd00d",
   "metadata": {},
   "source": [
    "## 2000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57f80d-5aae-488d-b3cd-670d521cba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_2000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=2000, num_train=100)\n",
    "    our_2000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbc50b-ca36-4c0a-bd8a-317c816c5a71",
   "metadata": {},
   "source": [
    "## 3000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ab090-4488-4e50-ae70-cceb1b983d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_3000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=3000, num_train=100)\n",
    "    our_3000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37071ccc-2acd-4a6e-b60c-daf5a44cf041",
   "metadata": {},
   "source": [
    "## 5000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994dfa7-1d75-4f05-b723-743649e6a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_5000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=5000, num_train=100)\n",
    "    our_5000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc26205-662f-4747-8ac6-81db4f576397",
   "metadata": {},
   "source": [
    "# SPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae73d54-bb91-4499-9a0b-16d2d2ba356b",
   "metadata": {},
   "source": [
    "## 100 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fa9a6-d0ae-45ab-a247-f3c95f5b899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_100_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=100, num_train=100)\n",
    "    other_100_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8053a8d-9e09-4e00-8b65-41096ece775e",
   "metadata": {},
   "source": [
    "## 150 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae551e2f-198b-4aa8-a35b-7d7dd6530212",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_150_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=150, num_train=100)\n",
    "    other_150_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099961c-978c-4749-ad4a-35223efde45b",
   "metadata": {},
   "source": [
    "## 200 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9867b-1b94-4597-be0f-40d6d0625e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_200_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=200, num_train=100)\n",
    "    other_200_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f0aff-4f6f-427f-82ac-3daa615d98d7",
   "metadata": {},
   "source": [
    "## 500 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9d651-7cb2-4e92-b093-72ca0111c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_500_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=500, num_train=100)\n",
    "    other_500_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a561780-04fa-4be3-a65c-74e4b1501d04",
   "metadata": {},
   "source": [
    "## 1000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e422b7-43e9-4f61-a12b-863c4150b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_1000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=1000, num_train=100)\n",
    "    other_1000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85f392-0922-41c9-89dc-fa1af23df483",
   "metadata": {},
   "source": [
    "## 2000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f18613-6c63-41cb-b15f-5b2aa6fc1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_2000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=2000, num_train=100)\n",
    "    other_2000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d0cee-c0a0-4ca5-84c9-ad05a40b2b36",
   "metadata": {},
   "source": [
    "## 3000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8edbab-e9fe-4450-a6c8-e71166dfdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_3000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=3000, num_train=100)\n",
    "    other_3000_theta_formal.append(theta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e332c6f-7746-477d-b450-4383c1c46959",
   "metadata": {},
   "source": [
    "## 5000 trajectorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37086eac-3370-4cf1-b06c-873ec830bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_5000_theta_formal = []\n",
    "\n",
    "for num in range(5):\n",
    "    print(f'Number Of Iteration:: {num}')\n",
    "    theta, theta_ls, grad_ls = sampled_based_train(theta=np.copy(THETA), lr=0.2, alpha=0.9, episode=5000, num_train=100)\n",
    "    other_5000_theta_formal.append(theta_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
